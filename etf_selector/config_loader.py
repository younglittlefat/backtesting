"""
é…ç½®æ–‡ä»¶åŠ è½½å™¨

æ”¯æŒä»JSONé…ç½®æ–‡ä»¶åŠ è½½å®Œæ•´å‚æ•°ï¼Œå¹¶ä¸CLIå‚æ•°åˆå¹¶
"""
import json
from pathlib import Path
from typing import Dict, Any, Optional
from etf_selector.config import FilterConfig, IndustryKeywords


class ConfigLoader:
    """é…ç½®æ–‡ä»¶åŠ è½½å™¨"""

    # JSON key â†’ FilterConfig field æ˜ å°„è¡¨
    KEY_MAPPING = {
        # Paths
        'paths.data_dir': 'data_dir',
        'paths.output_dir': 'output_dir',
        'paths.output_filename': 'output_filename',

        # Time range
        'time_range.start_date': 'start_date',
        'time_range.end_date': 'end_date',

        # Stage 1: Initial filter
        'stage1_initial_filter.min_turnover': 'min_turnover',
        'stage1_initial_filter.min_listing_days': 'min_listing_days',
        'stage1_initial_filter.turnover_lookback_days': 'turnover_lookback_days',

        # Stage 2: Core filter - ADX
        'stage2_core_filter.adx.period': 'adx_period',
        'stage2_core_filter.adx.lookback_days': 'adx_lookback_days',
        'stage2_core_filter.adx.percentile': 'adx_percentile',

        # Stage 2: Core filter - MA backtest
        'stage2_core_filter.ma_backtest.enable': 'enable_ma_backtest_filter',
        'stage2_core_filter.ma_backtest.short_period': 'ma_short',
        'stage2_core_filter.ma_backtest.long_period': 'ma_long',
        'stage2_core_filter.ma_backtest.ret_dd_percentile': 'ret_dd_percentile',

        # Stage 2: Core filter - Volatility
        'stage2_core_filter.volatility.min': 'min_volatility',
        'stage2_core_filter.volatility.max': 'max_volatility',
        'stage2_core_filter.volatility.lookback_days': 'volatility_lookback_days',

        # Stage 2: Core filter - Momentum
        'stage2_core_filter.momentum.periods': 'momentum_periods',
        'stage2_core_filter.momentum.min_positive': 'momentum_min_positive',

        # Stage 2: Filter mode control
        'stage2_core_filter.filter_mode.skip_percentile_filtering': 'skip_stage2_percentile_filtering',
        'stage2_core_filter.filter_mode.skip_range_filtering': 'skip_stage2_range_filtering',

        # Scoring system
        'scoring_system.enable_unbiased_scoring': 'enable_unbiased_scoring',
        'scoring_system.mode': 'score_mode',  # Special: convert to use_optimized_score
        'scoring_system.benchmark.ts_code': 'benchmark_ts_code',

        # Scoring windows
        'scoring_system.windows.excess_return_short': 'excess_return_short_window',
        'scoring_system.windows.excess_return_long': 'excess_return_long_window',
        'scoring_system.windows.trend_quality': 'trend_quality_window',
        'scoring_system.windows.trend_consistency': 'trend_consistency_window',
        'scoring_system.windows.price_efficiency': 'price_efficiency_window',
        'scoring_system.windows.volume_short': 'volume_short_window',
        'scoring_system.windows.volume_long': 'volume_long_window',
        'scoring_system.windows.liquidity_score': 'liquidity_score_window',

        # Weights V2 (optimized mode)
        'scoring_system.weights_v2.core_trend': 'core_trend_weight',
        'scoring_system.weights_v2.trend_quality': 'trend_quality_weight',
        'scoring_system.weights_v2.strength': 'strength_weight',
        'scoring_system.weights_v2.volume': 'volume_weight',
        'scoring_system.weights_v2.idr': 'idr_weight',
        'scoring_system.weights_v2.core_trend_sub.excess_return_20d': 'excess_return_20d_weight',
        'scoring_system.weights_v2.core_trend_sub.excess_return_60d': 'excess_return_60d_weight',

        # Weights V1 (legacy mode)
        'scoring_system.weights_v1_legacy.primary': 'primary_weight',
        'scoring_system.weights_v1_legacy.secondary': 'secondary_weight',
        'scoring_system.weights_v1_legacy.adx_score': 'adx_score_weight',
        'scoring_system.weights_v1_legacy.trend_consistency': 'trend_consistency_weight',
        'scoring_system.weights_v1_legacy.price_efficiency': 'price_efficiency_weight',
        'scoring_system.weights_v1_legacy.liquidity_score': 'liquidity_score_weight',
        'scoring_system.weights_v1_legacy.momentum_3m': 'momentum_3m_score_weight',
        'scoring_system.weights_v1_legacy.momentum_12m': 'momentum_12m_score_weight',

        # Stage 3: Diversification
        'stage3_diversification.target_portfolio_size': 'target_portfolio_size',
        'stage3_diversification.max_correlation': 'max_correlation',
        'stage3_diversification.min_industries': 'min_industries',

        # Stage 3: Deduplication
        'stage3_diversification.deduplication.enable': 'enable_deduplication',
        'stage3_diversification.deduplication.min_ratio': 'dedup_min_ratio',
        'stage3_diversification.deduplication.thresholds': 'dedup_thresholds',

        # Stage 3: Diversify V2
        'stage3_diversification.diversify_v2.enable': 'diversify_v2',
        'stage3_diversification.diversify_v2.score_diff_threshold': 'score_diff_threshold',

        # Stage 3: Industry balance
        'stage3_diversification.balance_industries': 'balance_industries',

        # Output options
        'output_options.verbose': 'verbose',
        'output_options.with_analysis': 'with_analysis',
        'output_options.skip_portfolio_optimization': 'skip_portfolio_optimization',
    }

    @staticmethod
    def load_from_json(json_path: str) -> FilterConfig:
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®

        Args:
            json_path: JSONé…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            FilterConfigå¯¹è±¡

        Raises:
            FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–éªŒè¯å¤±è´¥
        """
        path = Path(json_path)
        if not path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")

        try:
            with open(path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")

        # Flatten nested dictionary
        flat_dict = ConfigLoader._flatten_dict(config_dict)

        # Map JSON keys to FilterConfig fields
        mapped_dict = ConfigLoader._map_json_keys(flat_dict)

        # Create config object
        try:
            config = FilterConfig(**mapped_dict)
        except TypeError as e:
            raise ValueError(f"é…ç½®å‚æ•°é”™è¯¯: {e}")

        # Validate configuration
        ConfigLoader.validate(config)

        return config

    @staticmethod
    def _flatten_dict(nested_dict: Dict, parent_key: str = '', sep: str = '.') -> Dict:
        """æ‰å¹³åŒ–åµŒå¥—å­—å…¸

        Example:
            {'stage1': {'min_turnover': 50000}}
            â†’ {'stage1.min_turnover': 50000}

        Args:
            nested_dict: åµŒå¥—å­—å…¸
            parent_key: çˆ¶é”®å‰ç¼€
            sep: åˆ†éš”ç¬¦

        Returns:
            æ‰å¹³åŒ–åçš„å­—å…¸
        """
        items = []
        for k, v in nested_dict.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k

            # ç‰¹æ®Šå¤„ç†ï¼škeywordså­—å…¸ã€core_trend_subéœ€è¦ä¿æŒåµŒå¥—
            if isinstance(v, dict) and k not in ['keywords', 'core_trend_sub']:
                items.extend(ConfigLoader._flatten_dict(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))

        return dict(items)

    @staticmethod
    def _map_json_keys(flat_dict: Dict) -> Dict:
        """å°†JSONé”®æ˜ å°„åˆ°FilterConfigå­—æ®µå

        Args:
            flat_dict: æ‰å¹³åŒ–åçš„å­—å…¸

        Returns:
            æ˜ å°„åçš„å­—å…¸ï¼Œé”®ä¸ºFilterConfigå­—æ®µå
        """
        result = {}

        for json_key, value in flat_dict.items():
            # Skip metadata fields
            if json_key in ['version', 'description']:
                continue

            # Skip industry keywords (not part of FilterConfig, uses DEFAULT_INDUSTRY_KEYWORDS instead)
            if json_key == 'industry_classification.keywords':
                continue

            # Map using KEY_MAPPING
            if json_key in ConfigLoader.KEY_MAPPING:
                config_field = ConfigLoader.KEY_MAPPING[json_key]

                # Special handling for score_mode
                if config_field == 'score_mode':
                    result['use_optimized_score'] = (value == 'optimized')
                else:
                    result[config_field] = value
            else:
                # Unknown key - print warning but don't fail
                print(f"âš ï¸ æœªçŸ¥é…ç½®é”®: {json_key}")

        return result

    @staticmethod
    def validate(config: FilterConfig):
        """éªŒè¯é…ç½®å‚æ•°

        Raises:
            ValueError: é…ç½®éªŒè¯å¤±è´¥
        """
        errors = []

        # Validate V2 weights (if using optimized mode)
        if config.use_optimized_score:
            v2_weights_sum = (
                config.core_trend_weight +
                config.trend_quality_weight +
                config.strength_weight +
                config.volume_weight +
                config.idr_weight
            )
            if abs(v2_weights_sum - 1.0) > 0.01:
                errors.append(
                    f"V2æƒé‡æ€»å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{v2_weights_sum:.4f}"
                )

            core_trend_sub_sum = (
                config.excess_return_20d_weight +
                config.excess_return_60d_weight
            )
            if abs(core_trend_sub_sum - 1.0) > 0.01:
                errors.append(
                    f"æ ¸å¿ƒè¶‹åŠ¿å­æƒé‡æ€»å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{core_trend_sub_sum:.4f}"
                )

        # Validate percentile ranges
        if not (0 <= config.adx_percentile <= 100):
            errors.append(
                f"adx_percentileå¿…é¡»åœ¨[0, 100]èŒƒå›´å†…ï¼Œå½“å‰ä¸º{config.adx_percentile}"
            )

        if not (0 <= config.ret_dd_percentile <= 100):
            errors.append(
                f"ret_dd_percentileå¿…é¡»åœ¨[0, 100]èŒƒå›´å†…ï¼Œå½“å‰ä¸º{config.ret_dd_percentile}"
            )

        # Validate MA periods
        if config.ma_short >= config.ma_long:
            errors.append(
                f"ma_short ({config.ma_short})å¿…é¡»å°äºma_long ({config.ma_long})"
            )

        # Validate volatility range
        if config.min_volatility >= config.max_volatility:
            errors.append(
                f"min_volatility ({config.min_volatility})å¿…é¡»å°äºmax_volatility ({config.max_volatility})"
            )

        # Validate correlation threshold
        if not (0 <= config.max_correlation <= 1):
            errors.append(
                f"max_correlationå¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰ä¸º{config.max_correlation}"
            )

        # Validate positive integers
        if config.target_portfolio_size <= 0:
            errors.append(f"target_portfolio_sizeå¿…é¡»å¤§äº0ï¼Œå½“å‰ä¸º{config.target_portfolio_size}")

        if config.min_listing_days < 0:
            errors.append(f"min_listing_daysä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå½“å‰ä¸º{config.min_listing_days}")

        if errors:
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"  - {e}" for e in errors))

    @staticmethod
    def merge_with_cli_args(config: FilterConfig, args) -> FilterConfig:
        """å°†CLIå‚æ•°åˆå¹¶åˆ°é…ç½®ä¸­ï¼ˆCLIä¼˜å…ˆçº§æœ€é«˜ï¼‰

        Args:
            config: åŸºç¡€é…ç½®å¯¹è±¡
            args: argparse.Namespaceå‘½ä»¤è¡Œå‚æ•°

        Returns:
            åˆå¹¶åçš„é…ç½®å¯¹è±¡
        """
        # CLIå‚æ•°è¦†ç›–ï¼ˆä½¿ç”¨getattrå®‰å…¨è·å–ï¼‰
        # ä½¿ç”¨argparse.SUPPRESSåï¼Œæœªæ˜¾å¼ä¼ é€’çš„å‚æ•°ä¸ä¼šå‡ºç°åœ¨argsä¸­ï¼Œgetattrè¿”å›None
        cli_overrides = {
            'min_turnover': getattr(args, 'min_turnover', None),
            'min_listing_days': getattr(args, 'min_listing_days', None),
            'adx_percentile': getattr(args, 'adx_percentile', None),
            'ret_dd_percentile': getattr(args, 'ret_dd_percentile', None),
            'min_volatility': getattr(args, 'min_volatility', None),
            'max_volatility': getattr(args, 'max_volatility', None),
            'ma_short': getattr(args, 'ma_short', None),
            'ma_long': getattr(args, 'ma_long', None),
            'adx_period': getattr(args, 'adx_period', None),
            'target_portfolio_size': getattr(args, 'target_size', None),
            'max_correlation': getattr(args, 'max_correlation', None),
            'data_dir': getattr(args, 'data_dir', None),
            'dedup_min_ratio': getattr(args, 'dedup_min_ratio', None),
            'score_diff_threshold': getattr(args, 'score_diff_threshold', None),
        }

        # Apply overrides
        for key, value in cli_overrides.items():
            if value is not None:
                setattr(config, key, value)

        # Handle boolean flags
        if getattr(args, 'enable_ma_filter', False):
            config.enable_ma_backtest_filter = True
        elif getattr(args, 'disable_ma_filter', False):
            config.enable_ma_backtest_filter = False

        if getattr(args, 'disable_unbiased_scoring', False):
            config.enable_unbiased_scoring = False
        elif getattr(args, 'enable_unbiased_scoring', False):
            config.enable_unbiased_scoring = True

        if hasattr(args, 'score_mode') and args.score_mode:
            config.use_optimized_score = (args.score_mode == 'optimized')

        if getattr(args, 'momentum_min_positive', False):
            config.momentum_min_positive = True

        if getattr(args, 'skip_stage2_filtering', False):
            config.skip_stage2_percentile_filtering = True

        # diversify_v2æ˜¯action='store_true'ï¼Œæ‰€ä»¥ç›´æ¥æ£€æŸ¥
        if getattr(args, 'diversify_v2', False):
            config.diversify_v2 = True

        # Re-validate after merge
        ConfigLoader.validate(config)

        return config

    @staticmethod
    def print_all_params(config: FilterConfig, title: str = "å®Œæ•´é…ç½®å‚æ•°"):
        """æ‰“å°æ‰€æœ‰é…ç½®å‚æ•°ï¼ˆç”¨äºè°ƒè¯•å’ŒéªŒæ”¶ï¼‰

        Args:
            config: é…ç½®å¯¹è±¡
            title: æ ‡é¢˜
        """
        print("=" * 80)
        print(f" {title}")
        print("=" * 80)
        print()

        print("ğŸ“ è·¯å¾„é…ç½®:")
        print(f"  data_dir: {config.data_dir}")
        print(f"  output_dir: {config.output_dir}")
        print()

        print("ğŸ” ç¬¬ä¸€çº§ - åˆç­›å‚æ•°:")
        print(f"  min_turnover: {config.min_turnover:,.0f} å…ƒ")
        print(f"  min_listing_days: {config.min_listing_days} å¤©")
        print(f"  turnover_lookback_days: {config.turnover_lookback_days} å¤©")
        print()

        print("ğŸ¯ ç¬¬äºŒçº§ - æ ¸å¿ƒç­›é€‰å‚æ•°:")
        print(f"  ADX:")
        print(f"    period: {config.adx_period}")
        print(f"    lookback_days: {config.adx_lookback_days}")
        print(f"    percentile: {config.adx_percentile}% (ä¿ç•™å‰{100-config.adx_percentile:.0f}%)")
        print(f"  åŒå‡çº¿å›æµ‹:")
        print(f"    enable: {config.enable_ma_backtest_filter}")
        print(f"    ma_short: {config.ma_short}")
        print(f"    ma_long: {config.ma_long}")
        print(f"    ret_dd_percentile: {config.ret_dd_percentile}%")
        print(f"  æ³¢åŠ¨ç‡:")
        print(f"    min: {config.min_volatility:.2f}")
        print(f"    max: {config.max_volatility:.2f}")
        print(f"    lookback_days: {config.volatility_lookback_days}")
        print(f"  åŠ¨é‡:")
        print(f"    periods: {config.momentum_periods}")
        print(f"    min_positive: {config.momentum_min_positive}")
        print(f"  ç­›é€‰æ¨¡å¼:")
        print(f"    skip_stage2_percentile_filtering: {config.skip_stage2_percentile_filtering}")
        print(f"    skip_stage2_range_filtering: {config.skip_stage2_range_filtering}")
        print()

        print("ğŸ“Š è¯„åˆ†ç³»ç»Ÿ:")
        print(f"  enable_unbiased_scoring: {config.enable_unbiased_scoring}")
        print(f"  mode: {'optimized' if config.use_optimized_score else 'legacy'}")
        print(f"  benchmark_ts_code: {config.benchmark_ts_code}")
        print(f"  çª—å£å‚æ•°:")
        print(f"    excess_return_short_window: {config.excess_return_short_window}")
        print(f"    excess_return_long_window: {config.excess_return_long_window}")
        print(f"    trend_quality_window: {config.trend_quality_window}")
        print(f"    trend_consistency_window: {config.trend_consistency_window}")
        print(f"    price_efficiency_window: {config.price_efficiency_window}")
        print(f"    volume_short_window: {config.volume_short_window}")
        print(f"    volume_long_window: {config.volume_long_window}")
        print(f"    liquidity_score_window: {config.liquidity_score_window}")

        if config.use_optimized_score:
            print(f"  V2æƒé‡ (ä¼˜åŒ–ç‰ˆ):")
            print(f"    core_trend_weight: {config.core_trend_weight:.2f}")
            print(f"    trend_quality_weight: {config.trend_quality_weight:.2f}")
            print(f"    strength_weight: {config.strength_weight:.2f}")
            print(f"    volume_weight: {config.volume_weight:.2f}")
            print(f"    idr_weight: {config.idr_weight:.2f}")
            print(f"    æ ¸å¿ƒè¶‹åŠ¿å­æƒé‡:")
            print(f"      excess_return_20d_weight: {config.excess_return_20d_weight:.2f}")
            print(f"      excess_return_60d_weight: {config.excess_return_60d_weight:.2f}")
        else:
            print(f"  V1æƒé‡ (æ—§ç‰ˆ):")
            print(f"    primary_weight: {config.primary_weight:.2f}")
            print(f"    secondary_weight: {config.secondary_weight:.2f}")
            print(f"    adx_score_weight: {config.adx_score_weight:.2f}")
            print(f"    trend_consistency_weight: {config.trend_consistency_weight:.2f}")
            print(f"    price_efficiency_weight: {config.price_efficiency_weight:.2f}")
            print(f"    liquidity_score_weight: {config.liquidity_score_weight:.2f}")
            print(f"    momentum_3m_score_weight: {config.momentum_3m_score_weight:.2f}")
            print(f"    momentum_12m_score_weight: {config.momentum_12m_score_weight:.2f}")
        print()

        print("ğŸ² ç¬¬ä¸‰çº§ - åˆ†æ•£åŒ–å‚æ•°:")
        print(f"  target_portfolio_size: {config.target_portfolio_size}")
        print(f"  max_correlation: {config.max_correlation}")
        print(f"  min_industries: {config.min_industries}")

        # å¦‚æœæœ‰dedup_thresholdså±æ€§ï¼ˆæ–°å¢å­—æ®µï¼‰
        if hasattr(config, 'enable_deduplication'):
            print(f"  å»é‡:")
            print(f"    enable: {config.enable_deduplication}")
            if hasattr(config, 'dedup_min_ratio'):
                print(f"    min_ratio: {config.dedup_min_ratio}")
            if hasattr(config, 'dedup_thresholds'):
                print(f"    thresholds: {config.dedup_thresholds}")

        if hasattr(config, 'diversify_v2'):
            print(f"  V2åˆ†æ•£é€»è¾‘:")
            print(f"    enable: {config.diversify_v2}")
            if hasattr(config, 'score_diff_threshold'):
                print(f"    score_diff_threshold: {config.score_diff_threshold}")

        if hasattr(config, 'balance_industries'):
            print(f"  balance_industries: {config.balance_industries}")

        print()
        print("=" * 80)
