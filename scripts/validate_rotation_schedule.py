#!/usr/bin/env python3
"""
è½®åŠ¨è¡¨éªŒè¯è„šæœ¬

å¯¹ç”Ÿæˆçš„è½®åŠ¨è¡¨è¿›è¡Œå…¨é¢çš„è´¨é‡æ£€æŸ¥å’Œç»Ÿè®¡åˆ†æï¼Œç¡®ä¿æ•°æ®åˆç†æ€§ã€‚

éªŒè¯é¡¹ç›®ï¼š
1. æ–‡ä»¶å®Œæ•´æ€§ï¼šJSONæ ¼å¼ã€å¿…éœ€å­—æ®µ
2. æ•°æ®åˆç†æ€§ï¼šæ—¥æœŸé¡ºåºã€æ± å­å¤§å°ã€ETFä»£ç æ ¼å¼
3. ç»Ÿè®¡åˆ†æï¼šæ¢æ‰‹ç‡åˆ†å¸ƒã€ç¨³å®šæ€§åˆ†æ
4. å¯è§†åŒ–ï¼šè½®åŠ¨çƒ­åŠ›å›¾ã€æ¢æ‰‹ç‡è¶‹åŠ¿

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python scripts/validate_rotation_schedule.py results/rotation_schedules/rotation_30d.json
"""

import argparse
import json
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd


def load_schedule(file_path: Path) -> Dict:
    """åŠ è½½è½®åŠ¨è¡¨JSONæ–‡ä»¶

    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„

    Returns:
        è½®åŠ¨è¡¨æ•°æ®å­—å…¸

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        json.JSONDecodeError: JSONæ ¼å¼é”™è¯¯
    """
    if not file_path.exists():
        raise FileNotFoundError(f"è½®åŠ¨è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


def validate_structure(data: Dict) -> Tuple[bool, List[str]]:
    """éªŒè¯JSONç»“æ„å®Œæ•´æ€§

    Args:
        data: è½®åŠ¨è¡¨æ•°æ®

    Returns:
        (æ˜¯å¦é€šè¿‡, é”™è¯¯åˆ—è¡¨)
    """
    errors = []

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_keys = ['metadata', 'schedule', 'statistics']
    for key in required_keys:
        if key not in data:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {key}")

    # æ£€æŸ¥metadataå­—æ®µ
    if 'metadata' in data:
        meta = data['metadata']
        meta_required = ['rotation_period', 'pool_size', 'start_date', 'end_date', 'total_rotations']
        for key in meta_required:
            if key not in meta:
                errors.append(f"metadataç¼ºå°‘å­—æ®µ: {key}")

    # æ£€æŸ¥scheduleéç©º
    if 'schedule' in data and len(data['schedule']) == 0:
        errors.append("scheduleä¸ºç©º")

    return len(errors) == 0, errors


def validate_dates(schedule: Dict[str, List[str]], rotation_period: int) -> Tuple[bool, List[str]]:
    """éªŒè¯æ—¥æœŸåºåˆ—åˆç†æ€§

    Args:
        schedule: è½®åŠ¨æ—¶é—´è¡¨
        rotation_period: è½®åŠ¨å‘¨æœŸï¼ˆå¤©ï¼‰

    Returns:
        (æ˜¯å¦é€šè¿‡, é”™è¯¯åˆ—è¡¨)
    """
    errors = []
    dates = sorted(schedule.keys())

    # æ£€æŸ¥æ—¥æœŸæ ¼å¼
    for date_str in dates:
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
        except ValueError:
            errors.append(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}")

    # æ£€æŸ¥æ—¥æœŸé—´éš”ï¼ˆå…è®¸Â±2å¤©è¯¯å·®ï¼Œå› ä¸ºå¯èƒ½é‡åˆ°å‘¨æœ«/èŠ‚å‡æ—¥ï¼‰
    for i in range(1, len(dates)):
        prev_date = datetime.strptime(dates[i-1], '%Y-%m-%d')
        curr_date = datetime.strptime(dates[i], '%Y-%m-%d')
        delta = (curr_date - prev_date).days

        if abs(delta - rotation_period) > 2:
            errors.append(f"æ—¥æœŸé—´éš”å¼‚å¸¸: {dates[i-1]} â†’ {dates[i]} (é—´éš”{delta}å¤©ï¼Œé¢„æœŸ{rotation_period}å¤©)")

    return len(errors) == 0, errors


def validate_etf_codes(schedule: Dict[str, List[str]], pool_size: int) -> Tuple[bool, List[str]]:
    """éªŒè¯ETFä»£ç åˆç†æ€§

    Args:
        schedule: è½®åŠ¨æ—¶é—´è¡¨
        pool_size: ç›®æ ‡æ± å­å¤§å°

    Returns:
        (æ˜¯å¦é€šè¿‡, é”™è¯¯åˆ—è¡¨)
    """
    errors = []

    for date, codes in schedule.items():
        # æ£€æŸ¥æ± å­å¤§å°ï¼ˆå…è®¸æ¯”ç›®æ ‡å°‘1-2åªï¼Œå› ä¸ºå¯èƒ½é€€å¸‚ï¼‰
        if len(codes) < pool_size - 2:
            errors.append(f"{date}: æ± å­è¿‡å° ({len(codes)}åªï¼Œé¢„æœŸ{pool_size}åª)")

        # æ£€æŸ¥ä»£ç æ ¼å¼ï¼ˆåº”ä¸ºXXXXXX.SZæˆ–XXXXXX.SHï¼‰
        for code in codes:
            if not (code.endswith('.SZ') or code.endswith('.SH')):
                errors.append(f"{date}: ETFä»£ç æ ¼å¼é”™è¯¯ ({code})")

        # æ£€æŸ¥é‡å¤
        if len(codes) != len(set(codes)):
            duplicates = [code for code in codes if codes.count(code) > 1]
            errors.append(f"{date}: å­˜åœ¨é‡å¤ä»£ç  ({set(duplicates)})")

    return len(errors) == 0, errors


def analyze_statistics(schedule: Dict[str, List[str]]) -> Dict:
    """æ·±å…¥ç»Ÿè®¡åˆ†æ

    Args:
        schedule: è½®åŠ¨æ—¶é—´è¡¨

    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    dates = sorted(schedule.keys())

    # æ¢æ‰‹ç‡åºåˆ—
    turnover_rates = []
    for i in range(1, len(dates)):
        old_set = set(schedule[dates[i-1]])
        new_set = set(schedule[dates[i]])
        turnover = len(old_set ^ new_set) / (2 * len(old_set))
        turnover_rates.append(turnover)

    # ETFå‡ºç°é¢‘ç‡
    all_etfs = []
    for codes in schedule.values():
        all_etfs.extend(codes)
    etf_counter = Counter(all_etfs)

    # ç¨³å®šæ€§åˆ†å±‚
    total_rotations = len(dates)
    stability_tiers = {
        'æ ¸å¿ƒæ±  (>=80%)': [code for code, cnt in etf_counter.items() if cnt >= total_rotations * 0.8],
        'ç¨³å®šæ±  (50-80%)': [code for code, cnt in etf_counter.items() if total_rotations * 0.5 <= cnt < total_rotations * 0.8],
        'è½®æ¢æ±  (20-50%)': [code for code, cnt in etf_counter.items() if total_rotations * 0.2 <= cnt < total_rotations * 0.5],
        'è¾¹ç¼˜æ±  (<20%)': [code for code, cnt in etf_counter.items() if cnt < total_rotations * 0.2]
    }

    return {
        'turnover_rates': turnover_rates,
        'turnover_mean': sum(turnover_rates) / len(turnover_rates),
        'turnover_std': (sum((x - sum(turnover_rates)/len(turnover_rates))**2 for x in turnover_rates) / len(turnover_rates)) ** 0.5,
        'turnover_min': min(turnover_rates),
        'turnover_max': max(turnover_rates),
        'unique_etfs': len(etf_counter),
        'etf_appearances': dict(etf_counter.most_common(10)),
        'stability_tiers': {k: len(v) for k, v in stability_tiers.items()},
        'stability_tier_details': stability_tiers
    }


def print_validation_report(
    file_path: Path,
    data: Dict,
    structure_ok: bool,
    structure_errors: List[str],
    date_ok: bool,
    date_errors: List[str],
    code_ok: bool,
    code_errors: List[str],
    stats: Dict
):
    """æ‰“å°éªŒæ”¶æŠ¥å‘Š

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        data: è½®åŠ¨è¡¨æ•°æ®
        structure_ok: ç»“æ„éªŒè¯ç»“æœ
        structure_errors: ç»“æ„é”™è¯¯åˆ—è¡¨
        date_ok: æ—¥æœŸéªŒè¯ç»“æœ
        date_errors: æ—¥æœŸé”™è¯¯åˆ—è¡¨
        code_ok: ä»£ç éªŒè¯ç»“æœ
        code_errors: ä»£ç é”™è¯¯åˆ—è¡¨
        stats: ç»Ÿè®¡ç»“æœ
    """
    print("=" * 80)
    print(f" è½®åŠ¨è¡¨éªŒæ”¶æŠ¥å‘Š")
    print("=" * 80)
    print(f"\nğŸ“ æ–‡ä»¶: {file_path}")
    print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {data['metadata'].get('generated_at', 'N/A')}")

    # åŸºæœ¬ä¿¡æ¯
    meta = data['metadata']
    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  è½®åŠ¨å‘¨æœŸ: {meta['rotation_period']} å¤©")
    print(f"  æ± å­å¤§å°: {meta['pool_size']} åª")
    print(f"  å›æµ‹åŒºé—´: {meta['start_date']} è‡³ {meta['end_date']}")
    print(f"  è½®åŠ¨æ¬¡æ•°: {meta['total_rotations']} æ¬¡")

    # éªŒè¯ç»“æœ
    print(f"\nâœ… éªŒè¯ç»“æœ:")
    print(f"  ç»“æ„å®Œæ•´æ€§: {'âœ… é€šè¿‡' if structure_ok else 'âŒ å¤±è´¥'}")
    print(f"  æ—¥æœŸåˆç†æ€§: {'âœ… é€šè¿‡' if date_ok else 'âŒ å¤±è´¥'}")
    print(f"  ä»£ç æ ¼å¼: {'âœ… é€šè¿‡' if code_ok else 'âŒ å¤±è´¥'}")

    # é”™è¯¯è¯¦æƒ…
    if not (structure_ok and date_ok and code_ok):
        print(f"\nâŒ é”™è¯¯è¯¦æƒ…:")
        for error in structure_errors + date_errors + code_errors:
            print(f"  - {error}")

    # ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“ˆ ç»Ÿè®¡åˆ†æ:")
    print(f"  å”¯ä¸€ETFæ•°é‡: {stats['unique_etfs']} åª")
    print(f"  å¹³å‡æ¢æ‰‹ç‡: {stats['turnover_mean']:.2%} (Â±{stats['turnover_std']:.2%})")
    print(f"  æ¢æ‰‹ç‡èŒƒå›´: {stats['turnover_min']:.2%} - {stats['turnover_max']:.2%}")

    print(f"\nğŸ† å‡ºç°æ¬¡æ•°Top 5:")
    for i, (code, cnt) in enumerate(list(stats['etf_appearances'].items())[:5], 1):
        pct = cnt / meta['total_rotations']
        print(f"  {i}. {code}: {cnt}/{meta['total_rotations']} æ¬¡ ({pct:.0%})")

    print(f"\nğŸ¯ ç¨³å®šæ€§åˆ†å±‚:")
    for tier, count in stats['stability_tiers'].items():
        print(f"  {tier}: {count} åª")
        if count > 0 and count <= 5:
            # æ˜¾ç¤ºå…·ä½“ä»£ç ï¼ˆå¦‚æœæ•°é‡ä¸å¤šï¼‰
            codes = stats['stability_tier_details'][tier][:5]
            print(f"    {', '.join(codes)}")

    # å¥åº·åº¦è¯„åˆ†
    print(f"\nğŸ“ å¥åº·åº¦è¯„ä¼°:")
    health_score = 0
    health_items = []

    # 1. æ¢æ‰‹ç‡åˆç†æ€§ï¼ˆ10-40%ä¸ºä½³ï¼‰
    if 0.10 <= stats['turnover_mean'] <= 0.40:
        health_score += 25
        health_items.append("âœ… æ¢æ‰‹ç‡é€‚ä¸­")
    else:
        health_items.append(f"âš ï¸  æ¢æ‰‹ç‡{'è¿‡é«˜' if stats['turnover_mean'] > 0.40 else 'è¿‡ä½'}")

    # 2. ç¨³å®šæ€§ï¼ˆæ ¸å¿ƒæ± åº”å 10-30%ï¼‰
    core_pct = stats['stability_tiers']['æ ¸å¿ƒæ±  (>=80%)'] / stats['unique_etfs']
    if 0.10 <= core_pct <= 0.30:
        health_score += 25
        health_items.append("âœ… æ ¸å¿ƒæ± æ¯”ä¾‹åˆç†")
    else:
        health_items.append(f"âš ï¸  æ ¸å¿ƒæ± æ¯”ä¾‹{'è¿‡é«˜' if core_pct > 0.30 else 'è¿‡ä½'}")

    # 3. å¤šæ ·æ€§ï¼ˆå”¯ä¸€ETFåº”>æ± å­å¤§å°çš„2å€ï¼‰
    if stats['unique_etfs'] >= meta['pool_size'] * 2:
        health_score += 25
        health_items.append("âœ… ETFå¤šæ ·æ€§å……è¶³")
    else:
        health_items.append("âš ï¸  ETFå¤šæ ·æ€§ä¸è¶³")

    # 4. éªŒè¯é€šè¿‡
    if structure_ok and date_ok and code_ok:
        health_score += 25
        health_items.append("âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
    else:
        health_items.append("âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥")

    print(f"  æ€»åˆ†: {health_score}/100")
    for item in health_items:
        print(f"  {item}")

    # å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if stats['turnover_mean'] > 0.40:
        print("  - æ¢æ‰‹ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´äº¤æ˜“æˆæœ¬è¿‡å¤§ï¼Œè€ƒè™‘å¢åŠ è½®åŠ¨å‘¨æœŸ")
    elif stats['turnover_mean'] < 0.10:
        print("  - æ¢æ‰‹ç‡è¿‡ä½å¯èƒ½æ— æ³•åŠæ—¶è°ƒæ•´æ± å­ï¼Œè€ƒè™‘å‡å°‘è½®åŠ¨å‘¨æœŸ")

    if core_pct > 0.30:
        print("  - æ ¸å¿ƒæ± æ¯”ä¾‹è¿‡é«˜ï¼Œæ± å­ç¼ºä¹çµæ´»æ€§ï¼Œè€ƒè™‘è°ƒæ•´ç­›é€‰å‚æ•°")
    elif core_pct < 0.10:
        print("  - æ ¸å¿ƒæ± æ¯”ä¾‹è¿‡ä½ï¼Œæ± å­è¿‡äºä¸ç¨³å®šï¼Œå¯èƒ½å½±å“ç­–ç•¥è¿è´¯æ€§")

    if health_score >= 75:
        print("  âœ… æ•´ä½“è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºå›æµ‹")
    elif health_score >= 50:
        print("  âš ï¸  æ•´ä½“è´¨é‡å°šå¯ï¼Œå»ºè®®ä¼˜åŒ–åä½¿ç”¨")
    else:
        print("  âŒ æ•´ä½“è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")

    print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='éªŒè¯è½®åŠ¨è¡¨è´¨é‡')
    parser.add_argument('file', type=str, help='è½®åŠ¨è¡¨JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯')

    args = parser.parse_args()
    file_path = Path(args.file)

    # åŠ è½½æ–‡ä»¶
    try:
        data = load_schedule(file_path)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return 1

    # éªŒè¯ç»“æ„
    structure_ok, structure_errors = validate_structure(data)

    if not structure_ok:
        print("âŒ ç»“æ„éªŒè¯å¤±è´¥:")
        for error in structure_errors:
            print(f"  - {error}")
        return 1

    # æå–æ•°æ®
    schedule = data['schedule']
    metadata = data['metadata']

    # éªŒè¯æ—¥æœŸ
    date_ok, date_errors = validate_dates(schedule, metadata['rotation_period'])

    # éªŒè¯ETFä»£ç 
    code_ok, code_errors = validate_etf_codes(schedule, metadata['pool_size'])

    # ç»Ÿè®¡åˆ†æ
    stats = analyze_statistics(schedule)

    # æ‰“å°æŠ¥å‘Š
    print_validation_report(
        file_path, data,
        structure_ok, structure_errors,
        date_ok, date_errors,
        code_ok, code_errors,
        stats
    )

    # è¿”å›ç 
    if structure_ok and date_ok and code_ok:
        return 0
    else:
        return 1


if __name__ == "__main__":
    sys.exit(main())
